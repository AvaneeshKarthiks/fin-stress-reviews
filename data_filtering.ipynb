{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd0c95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from typing import List, Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2143d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINRAD_RAW_URL = \"https://raw.githubusercontent.com/sohomghosh/FinRAD_Financial_Readability_Assessment_Dataset/main/data_sample_1500.csv\"\n",
    "FINRAD_TOP_K = 500\n",
    "MIN_TOKEN_LENGTH = 3\n",
    "MIN_REVIEW_LEN = 10\n",
    "FILENAME = \"binance_reviews_last5y.csv\"\n",
    "OUTPUT_PATH = FILENAME[:-4] + \"_filtered.csv\"\n",
    "\n",
    "COMMON_FIN_WORDS = {\n",
    "    \"buy\", \"sell\", \"trade\", \"order\", \"limit order\", \"market order\", \"stop loss\",\n",
    "    \"margin\", \"margin call\", \"intraday\", \"swing\", \"long\", \"short\",\n",
    "    \"stock\", \"share\", \"equity\", \"futures\", \"options\", \"mutual fund\", \"sip\", \"ipo\", \"etf\",\n",
    "    \"bond\", \"derivative\", \"portfolio\", \"broker\", \"brokerage\", \"commission\", \"fee\", \"tax\",\n",
    "    \"dividend\", \"profit\", \"loss\", \"pnl\", \"realized\", \"unrealized\", \"nse\", \"bse\", \"nifty\",\n",
    "    \"sensex\", \"ltp\", \"tick\", \"volume\", \"order id\", \"txn\", \"transaction\", \"txnid\",\n",
    "    \"settlement\", \"upi\", \"bank\", \"demat\", \"dp\", \"scrip\", \"circuit breaker\", \"selloff\",\n",
    "    \"panic\", \"panic-sell\", \"withdraw\", \"deposit\", \"portfolio value\", \"exit\", \"entry\",\n",
    "    \"brokerage\", \"tax-loss\", \"tax harvesting\", \"brokerage fee\", \"kyc\"\n",
    "}\n",
    "\n",
    "REGEX_PATTERNS = {\n",
    "    \"percent_value\": re.compile(r\"\\b\\d{1,3}(?:[.,]\\d+)?\\s?%\"),                        # 5% / 10.5 %\n",
    "    \"rupee_symbol_amount\": re.compile(r\"₹\\s?\\d[\\d,]*(?:\\.\\d+)?\"),                    # ₹1,234.56\n",
    "    \"rs_amount\": re.compile(r\"\\b(?:rs\\.?|inr)\\s?\\d[\\d,]*(?:\\.\\d+)?\\b\", re.I),         # rs 1200 / INR1200\n",
    "    \"exchange_tag\": re.compile(r\"\\b(?:NSE|BSE|NIFTY|SENSEX)\\b\", re.I),\n",
    "    \"order_txn\": re.compile(r\"\\b(?:order id|order#|ordernumber|txn id|transaction id|txnid)\\b\", re.I),\n",
    "    \"stop_loss_phrase\": re.compile(r\"\\bstop[- ]?loss\\b\", re.I),\n",
    "    \"margin_call_phrase\": re.compile(r\"\\bmargin call\\b\", re.I),\n",
    "    \"at_price_shorthand\": re.compile(r\"[@]\\s?\\d[\\d,]*(?:\\.\\d+)?\"),                    # '@ 450' or '@450'\n",
    "    \"percent_word\": re.compile(r\"\\bpercent\\b|\\bpercentage\\b\", re.I),\n",
    "    \"price_word\": re.compile(r\"\\bprice\\b|\\bclosing price\\b|\\bopen price\\b\", re.I),\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43463497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Helper functions ----------\n",
    "def download_finrad_csv(raw_url: str, timeout=30):\n",
    "    try:\n",
    "        headers = {\"User-Agent\": \"python-requests/2.x\"}\n",
    "        r = requests.get(raw_url, headers=headers, timeout=timeout)\n",
    "        r.raise_for_status()\n",
    "        return r.content\n",
    "    except Exception as e:\n",
    "        print(f\"[warning] could not download FinRAD from {raw_url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_finrad_df(raw_bytes: bytes):\n",
    "    try:\n",
    "        return pd.read_csv(io.BytesIO(raw_bytes), encoding=\"utf-8\", low_memory=False)\n",
    "    except Exception as e:\n",
    "        print(\"[warning] failed to parse FinRAD CSV:\", e)\n",
    "        return None\n",
    "\n",
    "# def choose_text_column(df: pd.DataFrame) -> str:\n",
    "#     preferred = [c for c in df.columns if c.lower() in {\"text\", \"sentence\", \"content\", \"sample\", \"excerpt\"}]\n",
    "#     if preferred:\n",
    "#         return preferred[0]\n",
    "#     # fallback: pick object dtype column with largest average length\n",
    "#     obj_cols = [c for c in df.columns if df[c].dtype == \"object\"]\n",
    "#     if not obj_cols:\n",
    "#         return df.columns[0]\n",
    "#     avg_lens = {c: df[c].astype(str).str.len().mean() for c in obj_cols}\n",
    "#     return max(avg_lens, key=avg_lens.get)\n",
    "\n",
    "def extract_top_tokens_from_texts(texts: List[str], top_k=500, min_len=3, stopwords: Set[str]=None):\n",
    "    token_re = re.compile(r\"\\b[a-zA-Z]{%d,}\\b\" % min_len)\n",
    "    counter = Counter()\n",
    "    for t in texts:\n",
    "        if not isinstance(t, str):\n",
    "            continue\n",
    "        for m in token_re.findall(t):\n",
    "            token = m.lower()\n",
    "            if stopwords and token in stopwords:\n",
    "                continue\n",
    "            counter[token] += 1\n",
    "    most = [tok for tok, _ in counter.most_common(top_k)]\n",
    "    return most\n",
    "\n",
    "def compile_keyword_regex(words: List[str]) -> re.Pattern:\n",
    "    escaped = [re.escape(w) for w in sorted(set(words), key=len, reverse=True) if w.strip()]\n",
    "    if not escaped:\n",
    "        # fallback to a safe small pattern\n",
    "        return re.compile(r\"\\b(?:buy|sell|trade|stock|share|sip)\\b\", re.I)\n",
    "    pattern = r\"\\b(?:\" + \"|\".join(escaped) + r\")\\b\"\n",
    "    return re.compile(pattern, re.I)\n",
    "\n",
    "def filter_reviews_by_whitelist_and_regex(reviews_df: pd.DataFrame, text_col: str, keyword_pattern: re.Pattern, regex_patterns: dict, min_len: int = 10):\n",
    "    s = reviews_df[text_col].astype(str).fillna(\"\").str.strip()\n",
    "    length_ok = s.str.len() >= min_len\n",
    "    is_keyword = s.str.contains(keyword_pattern, na=False)\n",
    "\n",
    "    matched_any_regex = pd.Series(False, index=reviews_df.index)\n",
    "    matched_regex_keys = [[] for _ in range(len(reviews_df))]\n",
    "    for key, pat in regex_patterns.items():\n",
    "        found = s.str.contains(pat, na=False)\n",
    "        matched_any_regex = matched_any_regex | found\n",
    "        for idx in reviews_df.index[found]:\n",
    "            matched_regex_keys[idx] = matched_regex_keys[idx] + [key]\n",
    "\n",
    "    out = reviews_df.copy()\n",
    "    out[\"text_len\"] = s.str.len()\n",
    "    out[\"is_keyword\"] = is_keyword\n",
    "    out[\"is_regex\"] = matched_any_regex\n",
    "    out[\"matched_regexes\"] = [\",\".join(keys) if keys else \"\" for keys in matched_regex_keys]\n",
    "    def first_keyword_match(txt: str):\n",
    "        m = keyword_pattern.search(txt)\n",
    "        return m.group(0) if m else \"\"\n",
    "    out[\"matched_keyword_snippet\"] = s.apply(first_keyword_match)\n",
    "    out[\"finance_flag\"] = (out[\"is_keyword\"] | out[\"is_regex\"]) & length_ok\n",
    "\n",
    "    def reason_row(r):\n",
    "        if not r[\"finance_flag\"]:\n",
    "            return \"no_match_or_too_short\"\n",
    "        if r[\"is_keyword\"]:\n",
    "            return f\"keyword:{r['matched_keyword_snippet'] or 'match'}\"\n",
    "        if r[\"is_regex\"]:\n",
    "            return f\"regex:{r['matched_regexes']}\"\n",
    "        return \"matched\"\n",
    "    out[\"reason\"] = out.apply(reason_row, axis=1)\n",
    "    return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a757c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(FILENAME):\n",
    "    raise FileNotFoundError(f\"'{FILENAME}' not found in working directory. Please ensure the file is present.\")\n",
    "df = pd.read_csv(FILENAME, low_memory=False)\n",
    "print(f\"Loaded '{FILENAME}' with {len(df)} rows and columns: {list(df.columns)[:20]}\")\n",
    "\n",
    "if \"content\" not in df.columns:\n",
    "    raise ValueError(\"Column 'content' not found in zerodha.csv. Please ensure your dataset has a 'content' column.\")\n",
    "\n",
    "finrad_bytes = download_finrad_csv(FINRAD_RAW_URL)\n",
    "finrad_tokens = []\n",
    "if finrad_bytes:\n",
    "    df_fin = load_finrad_df(finrad_bytes)\n",
    "    if df_fin is not None:\n",
    "        text_col = \"terms\"\n",
    "        fin_texts = df_fin[text_col].astype(str).tolist()\n",
    "        stopwords = set()\n",
    "        try:\n",
    "            import nltk\n",
    "            nltk.download('stopwords', quiet=True)\n",
    "            from nltk.corpus import stopwords as sw\n",
    "            stopwords = set(sw.words('english'))\n",
    "        except Exception:\n",
    "            stopwords = {\"the\", \"and\", \"for\", \"with\", \"that\", \"this\", \"are\", \"was\", \"from\", \"have\", \"has\"}\n",
    "        finrad_tokens = extract_top_tokens_from_texts(fin_texts, top_k=FINRAD_TOP_K, min_len=MIN_TOKEN_LENGTH, stopwords=stopwords)\n",
    "        print(f\"Extracted {len(finrad_tokens)} tokens from FinRAD (top {FINRAD_TOP_K}).\")\n",
    "    else:\n",
    "        print(\"Could not parse FinRAD CSV; continuing with curated common words only.\")\n",
    "else:\n",
    "    print(\"FinRAD download failed; continuing with curated common words only.\")\n",
    "\n",
    "merged = set(w.lower() for w in COMMON_FIN_WORDS)\n",
    "merged.update(finrad_tokens)\n",
    "merged_list = sorted(merged)\n",
    "print(f\"Merged whitelist size: {len(merged_list)} (including curated common words and FinRAD-derived tokens)\")\n",
    "\n",
    "keyword_re = compile_keyword_regex(merged_list)\n",
    "\n",
    "filtered_df = filter_reviews_by_whitelist_and_regex(df, text_col=\"content\", keyword_pattern=keyword_re, regex_patterns=REGEX_PATTERNS, min_len=MIN_REVIEW_LEN)\n",
    "\n",
    "finance_rows = filtered_df[filtered_df[\"finance_flag\"]].copy()\n",
    "finance_rows.to_csv(OUTPUT_PATH, index=False)\n",
    "print(f\"Filtered dataset saved to: {OUTPUT_PATH}\")\n",
    "print(f\"Total rows: {len(df)}, Finance-related rows kept: {len(finance_rows)} ({len(finance_rows)/len(df)*100:.2f}%)\")\n",
    "\n",
    "display_cols = [\"content\", \"finance_flag\", \"is_keyword\", \"is_regex\", \"matched_keyword_snippet\", \"matched_regexes\", \"reason\"]\n",
    "sample_display = finance_rows[display_cols].head(20)\n",
    "print(sample_display.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bb9fd0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165185\n"
     ]
    }
   ],
   "source": [
    "print(len(filtered_df['content'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0edafbd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19063, 14)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finance_rows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031b8608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "NEGATIVE_WORDS = {\n",
    "    \"app\", \"application\", \"ui\", \"ux\", \"interface\", \"layout\", \"theme\", \"dark mode\", \"color\", \"design\",\n",
    "    \"button\", \"menu\", \"screen\", \"screen freeze\", \"freeze\", \"frozen\", \"render\", \"scroll\", \"tap\", \"click\",\n",
    "    \"slow\", \"lag\", \"lagging\", \"loading\", \"load\", \"responsive\", \"unresponsive\", \"crash\", \"crashes\", \"crashed\",\n",
    "    \"bug\", \"bugs\", \"glitch\", \"glitches\", \"error\", \"errors\", \"exception\",\n",
    "    \"login\", \"logout\", \"signin\", \"sign in\", \"signup\", \"sign up\", \"register\", \"registration\", \"password\",\n",
    "    \"forgot password\", \"otp\", \"two-factor\", \"2fa\", \"mfa\", \"biometric\", \"fingerprint\", \"faceid\",\n",
    "    \"pin\", \"pincode\", \"session expired\", \"session\", \"token\", \"auth\", \"authenticate\",\n",
    "    \"install\", \"uninstall\", \"update\", \"updated\", \"version\", \"play store\", \"app store\", \"rating\", \"stars\",\n",
    "    \"notification\", \"notifications\", \"permission\", \"permissions\", \"push\", \"push notification\",\n",
    "    \"support\", \"customer support\", \"help\", \"contact\", \"ticket\",\n",
    "    \"hamburger\", \"toolbar\", \"popup\", \"dialog\", \"modal\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c4f355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_phrase_regex(words):\n",
    "    escaped = [re.escape(w) for w in sorted(set(words), key=len, reverse=True) if w.strip()]\n",
    "    if not escaped:\n",
    "        return re.compile(r\"$^\", re.I)\n",
    "    pattern = r\"\\b(?:\" + \"|\".join(escaped) + r\")\\b\"\n",
    "    return re.compile(pattern, re.I)\n",
    "\n",
    "NEGATIVE_REGEX = compile_phrase_regex(NEGATIVE_WORDS)\n",
    "\n",
    "def negative_filter(finance_rows, text_col=\"content\", drop=True):\n",
    "\n",
    "    s = finance_rows[text_col].astype(str).fillna(\"\")\n",
    "    negative_matches = s.str.contains(NEGATIVE_REGEX, na=False)\n",
    "    out = finance_rows.copy()\n",
    "    out[\"negative_flag\"] = negative_matches\n",
    "    def _snippet(txt):\n",
    "        m = NEGATIVE_REGEX.search(txt)\n",
    "        return m.group(0) if m else \"\"\n",
    "    out[\"negative_match_snippet\"] = out[text_col].apply(_snippet)\n",
    "    if drop:\n",
    "        return out[~out[\"negative_flag\"]].reset_index(drop=True)\n",
    "    else:\n",
    "        return out.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298b3efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned finance-only reviews (UI/login removed): 9359 rows -> binance_reviews_last5y_filtered_no_ui.csv\n"
     ]
    }
   ],
   "source": [
    "final_df = negative_filter(finance_rows, text_col=\"content\", drop=True)\n",
    "\n",
    "final_csv_path = FILENAME[:-4] + \"_filtered_no_ui.csv\"\n",
    "final_df.to_csv(final_csv_path, index=False)\n",
    "\n",
    "print(f\"Saved cleaned finance-only reviews (UI/login removed): {len(final_df)} rows -> {final_csv_path}\")\n",
    "\n",
    "# flagged_df = negative_filter(finance_rows, text_col=\"content\", drop=False)\n",
    "# flagged_csv_path = \"zerodha_finance_filtered_flagged_ui.csv\"\n",
    "# flagged_df.to_csv(flagged_csv_path, index=False)\n",
    "# print(f\"Saved finance reviews with negative flags: {len(flagged_df)} rows -> {flagged_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "137635b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 21)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae0e7ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(final_df['content'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8592eaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import io\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from typing import List, Set\n",
    "\n",
    "FINRAD_RAW_URL = \"https://raw.githubusercontent.com/sohomghosh/FinRAD_Financial_Readability_Assessment_Dataset/main/data_sample_1500.csv\"\n",
    "finrad_local_path = None  # e.g., \"/path/to/data_sample_1500.csv\" to use a local copy instead of downloading\n",
    "\n",
    "TOP_K_FINRAD_TOKENS = 500   # how many top tokens to extract from FinRAD as candidate finance words\n",
    "MIN_TOKEN_LENGTH = 3        # ignore very short tokens\n",
    "MIN_REVIEW_LEN = 10         # minimum characters for a review to be considered\n",
    "# ------------------------------\n",
    "\n",
    "# ---------- Common curated financial words (compact set you can expand) ----------\n",
    "# This is a small curated \"common financial words\" set to combine with FinRAD-derived tokens.\n",
    "COMMON_FIN_WORDS = {\n",
    "    \"buy\", \"sell\", \"trade\", \"order\", \"limit order\", \"market order\", \"stop loss\",\n",
    "    \"margin\", \"margin call\", \"intraday\", \"swing\", \"long\", \"short\",\n",
    "    \"stock\", \"share\", \"equity\", \"futures\", \"options\", \"mutual fund\", \"sip\", \"ipo\", \"etf\",\n",
    "    \"bond\", \"derivative\", \"portfolio\", \"broker\", \"brokerage\", \"commission\", \"fee\", \"tax\",\n",
    "    \"dividend\", \"profit\", \"loss\", \"pnl\", \"realized\", \"unrealized\", \"nse\", \"bse\", \"nifty\",\n",
    "    \"sensex\", \"ipo allotment\", \"ltp\", \"tick\", \"volume\", \"order id\", \"txn\", \"transaction\",\n",
    "    \"settlement\", \"upi\", \"bank\", \"demat\", \"dp\", \"scrip\", \"circuit breaker\", \"selloff\",\n",
    "    \"panic\", \"panic-sell\", \"withdraw\", \"deposit\", \"portfolio value\", \"exit\", \"entry\",\n",
    "    \"brokerage\", \"tax-loss\", \"tax harvesting\"\n",
    "}\n",
    "\n",
    "# ---------- regex patterns ----------\n",
    "REGEX_PATTERNS = {\n",
    "    \"percent_value\": re.compile(r\"\\b\\d{1,3}(?:[.,]\\d+)?\\s?%\"),                        # 5% / 10.5 %\n",
    "    \"rupee_symbol_amount\": re.compile(r\"₹\\s?\\d[\\d,]*(?:\\.\\d+)?\"),                    # ₹1,234.56\n",
    "    \"rs_amount\": re.compile(r\"\\b(?:rs\\.?|inr)\\s?\\d[\\d,]*(?:\\.\\d+)?\\b\", re.I),         # rs 1200 / INR1200\n",
    "    \"exchange_tag\": re.compile(r\"\\b(?:NSE|BSE|NIFTY|SENSEX)\\b\", re.I),\n",
    "    \"order_txn\": re.compile(r\"\\b(?:order id|order#|ordernumber|txn id|transaction id|txnid)\\b\", re.I),\n",
    "    \"stop_loss_phrase\": re.compile(r\"\\bstop[- ]?loss\\b\", re.I),\n",
    "    \"margin_call_phrase\": re.compile(r\"\\bmargin call\\b\", re.I),\n",
    "    \"at_price_shorthand\": re.compile(r\"[@]\\s?\\d[\\d,]*(?:\\.\\d+)?\"),                    # '@ 450' or '@450'\n",
    "    \"percent_word\": re.compile(r\"\\bpercent\\b|\\bpercentage\\b\", re.I),\n",
    "    \"price_word\": re.compile(r\"\\bprice\\b|\\bclosing price\\b|\\bopen price\\b\", re.I),\n",
    "}\n",
    "\n",
    "# ---------- helper functions ----------\n",
    "def download_finrad_csv(raw_url: str) -> bytes:\n",
    "    \"\"\"Download the CSV bytes from the raw GitHub URL. If it fails, raise an exception.\"\"\"\n",
    "    headers = {\"User-Agent\": \"python-requests/2.x\"}\n",
    "    r = requests.get(raw_url, headers=headers, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    return r.content\n",
    "\n",
    "def load_finrad_df(raw_bytes: bytes) -> pd.DataFrame:\n",
    "    \"\"\"Load csv bytes into a pandas DataFrame.\"\"\"\n",
    "    return pd.read_csv(io.BytesIO(raw_bytes), encoding=\"utf-8\", low_memory=False)\n",
    "\n",
    "def choose_text_column(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Heuristic to pick the most likely text column in FinRAD sample:\n",
    "    prefer columns named 'text', 'sentence', 'content', otherwise choose the string column\n",
    "    with largest average length.\n",
    "    \"\"\"\n",
    "    preferred = [c for c in df.columns if c.lower() in {\"text\", \"sentence\", \"content\", \"sample\", \"excerpt\"}]\n",
    "    if preferred:\n",
    "        return preferred[0]\n",
    "    # fallback: pick object dtype column with largest average length\n",
    "    obj_cols = [c for c in df.columns if df[c].dtype == \"object\"]\n",
    "    if not obj_cols:\n",
    "        # as fallback, use first column\n",
    "        return df.columns[0]\n",
    "    avg_lens = {c: df[c].astype(str).str.len().mean() for c in obj_cols}\n",
    "    # return column with max avg len\n",
    "    return max(avg_lens, key=avg_lens.get)\n",
    "\n",
    "def extract_top_tokens_from_texts(texts: List[str], top_k=500, min_len=3, stopwords: Set[str]=None) -> List[str]:\n",
    "    \"\"\"\n",
    "    Tokenize texts, count token frequency, and return top_k tokens (lowercased).\n",
    "    Very conservative tokenization: keeps alphabetic tokens.\n",
    "    \"\"\"\n",
    "    token_re = re.compile(r\"\\b[a-zA-Z]{%d,}\\b\" % min_len)\n",
    "    counter = Counter()\n",
    "    for t in texts:\n",
    "        if not isinstance(t, str):\n",
    "            continue\n",
    "        for m in token_re.findall(t):\n",
    "            token = m.lower()\n",
    "            if stopwords and token in stopwords:\n",
    "                continue\n",
    "            counter[token] += 1\n",
    "    most = [tok for tok, _ in counter.most_common(top_k)]\n",
    "    return most\n",
    "\n",
    "def compile_keyword_regex(words: List[str]) -> re.Pattern:\n",
    "    \"\"\"\n",
    "    Build a case-insensitive regex that will match any phrase in words.\n",
    "    Long phrases are placed earlier to ensure greedy matching.\n",
    "    \"\"\"\n",
    "    # Escape and sort by length to match multi-word phrases first\n",
    "    escaped = [re.escape(w) for w in sorted(set(words), key=len, reverse=True) if w.strip()]\n",
    "    # join using a non-capturing group; use word boundaries to reduce false positives\n",
    "    pattern = r\"\\b(?:\" + \"|\".join(escaped) + r\")\\b\"\n",
    "    return re.compile(pattern, re.I)\n",
    "\n",
    "def filter_reviews_by_whitelist_and_regex(\n",
    "    reviews_df: pd.DataFrame,\n",
    "    text_col: str,\n",
    "    keyword_pattern: re.Pattern,\n",
    "    regex_patterns: dict,\n",
    "    min_len: int = 10\n",
    ") -> pd.DataFrame:\n",
    "    s = reviews_df[text_col].astype(str).fillna(\"\").str.strip()\n",
    "    length_ok = s.str.len() >= min_len\n",
    "    is_keyword = s.str.contains(keyword_pattern, na=False)\n",
    "\n",
    "    matched_any_regex = pd.Series(False, index=reviews_df.index)\n",
    "    matched_regex_keys = [[] for _ in range(len(reviews_df))]\n",
    "    for key, pat in regex_patterns.items():\n",
    "        found = s.str.contains(pat, na=False)\n",
    "        matched_any_regex = matched_any_regex | found\n",
    "        for idx in reviews_df.index[found]:\n",
    "            matched_regex_keys[idx] = matched_regex_keys[idx] + [key]\n",
    "\n",
    "    out = reviews_df.copy()\n",
    "    out[\"text_len\"] = s.str.len()\n",
    "    out[\"is_keyword\"] = is_keyword\n",
    "    out[\"is_regex\"] = matched_any_regex\n",
    "    out[\"matched_regexes\"] = [\",\".join(keys) if keys else \"\" for keys in matched_regex_keys]\n",
    "    # first keyword match snippet\n",
    "    def first_keyword_match(txt: str):\n",
    "        m = keyword_pattern.search(txt)\n",
    "        return m.group(0) if m else \"\"\n",
    "    out[\"matched_keyword_snippet\"] = s.apply(first_keyword_match)\n",
    "    out[\"finance_flag\"] = (out[\"is_keyword\"] | out[\"is_regex\"]) & length_ok\n",
    "\n",
    "    def reason_row(r):\n",
    "        if not r[\"finance_flag\"]:\n",
    "            return \"no_match_or_too_short\"\n",
    "        if r[\"is_keyword\"]:\n",
    "            return f\"keyword:{r['matched_keyword_snippet'] or 'match'}\"\n",
    "        if r[\"is_regex\"]:\n",
    "            return f\"regex:{r['matched_regexes']}\"\n",
    "        return \"matched\"\n",
    "    out[\"reason\"] = out.apply(reason_row, axis=1)\n",
    "    return out\n",
    "\n",
    "# ---------- Main pipeline ----------\n",
    "def build_and_apply_filter(reviews_df: pd.DataFrame, review_text_col: str = \"review\"):\n",
    "    # 1) load FinRAD tokens\n",
    "    finrad_texts = []\n",
    "    try:\n",
    "        if finrad_local_path:\n",
    "            with open(finrad_local_path, \"rb\") as f:\n",
    "                raw = f.read()\n",
    "        else:\n",
    "            raw = download_finrad_csv(FINRAD_RAW_URL)\n",
    "        df_finrad = load_finrad_df(raw)\n",
    "        text_col = \"terms\"\n",
    "        print(\"FinRAD: using text column:\", text_col)\n",
    "        finrad_texts = df_finrad[text_col].astype(str).tolist()\n",
    "    except Exception as e:\n",
    "        print(\"Could not load FinRAD CSV automatically:\", e)\n",
    "        finrad_texts = []\n",
    "\n",
    "    # 2) compute top tokens from FinRAD (if available)\n",
    "    stopwords = set()\n",
    "    try:\n",
    "        import nltk\n",
    "        nltk.download('stopwords', quiet=True)\n",
    "        from nltk.corpus import stopwords as sw\n",
    "        stopwords = set(sw.words('english'))\n",
    "    except Exception:\n",
    "        # if nltk not available, use a small default stopwords set\n",
    "        stopwords = {\"the\", \"and\", \"for\", \"with\", \"that\", \"this\", \"are\", \"was\", \"from\", \"have\", \"has\"}\n",
    "\n",
    "    fin_tokens = []\n",
    "    if finrad_texts:\n",
    "        fin_tokens = extract_top_tokens_from_texts(finrad_texts, top_k=TOP_K_FINRAD_TOKENS, min_len=MIN_TOKEN_LENGTH, stopwords=stopwords)\n",
    "        print(f\"Extracted {len(fin_tokens)} tokens from FinRAD (top {TOP_K_FINRAD_TOKENS})\")\n",
    "    else:\n",
    "        print(\"No FinRAD texts available; skipping FinRAD-derived tokens.\")\n",
    "\n",
    "    # 3) Merge curated common list + finrad tokens\n",
    "    merged = set([w.lower() for w in COMMON_FIN_WORDS])\n",
    "    merged.update(fin_tokens)\n",
    "    merged_list = sorted(merged)\n",
    "    print(\"Merged whitelist size:\", len(merged_list))\n",
    "\n",
    "    # 4) compile keyword regex\n",
    "    keyword_re = compile_keyword_regex(merged_list)\n",
    "\n",
    "    # 5) apply filter to user's reviews DataFrame\n",
    "    filtered = filter_reviews_by_whitelist_and_regex(reviews_df, text_col=review_text_col, keyword_pattern=keyword_re, regex_patterns=REGEX_PATTERNS, min_len=MIN_REVIEW_LEN)\n",
    "    return filtered, merged_list\n",
    "\n",
    "# ---------- Example usage ----------\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample reviews DF (replace with your Zerodha reviews DataFrame)\n",
    "    sample_reviews = [\n",
    "        \"Bought 100 shares of RELIANCE at ₹2,345.50 today\",\n",
    "        \"App crashes when placing order - please fix\",\n",
    "        \"Great UI, love the new color scheme!\",\n",
    "        \"SIP failed this month and I am worried about my investments\",\n",
    "        \"Login issue, can't access my account\",\n",
    "        \"Sold all my holdings after panic sell, huge loss of 12%\",\n",
    "        \"Brokerage fee seems high compared to other platforms\",\n",
    "        \"Why is the KYC pending for 2 days?\"\n",
    "    ]\n",
    "    reviews_df = pd.DataFrame({\"review\": sample_reviews})\n",
    "    filtered_df, whitelist = build_and_apply_filter(reviews_df, review_text_col=\"review\")\n",
    "\n",
    "    print(filtered_df[[\"review\", \"finance_flag\", \"is_keyword\", \"is_regex\", \"matched_keyword_snippet\", \"matched_regexes\", \"reason\"]])\n",
    "    # optionally save the whitelist\n",
    "    pd.Series(whitelist).to_csv(\"merged_finance_whitelist.csv\", index=False, header=False)\n",
    "    print(\"Saved merged whitelist to merged_finance_whitelist.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fin-stress-reviews",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
